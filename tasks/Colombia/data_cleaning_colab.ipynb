{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roman Numeral Recognition - Data Cleaning & Augmentation\n",
    "This notebook helps you improve the dataset quality to achieve >90% accuracy.\n",
    "\n",
    "## Setup Instructions for Google Colab:\n",
    "1. Upload this notebook to Google Colab\n",
    "2. Upload your `dataset.zip` (containing train/ and val/ folders)\n",
    "3. Run all cells sequentially\n",
    "4. Download the cleaned dataset and train locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pillow matplotlib numpy scikit-learn opencv-python -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "from sklearn.cluster import DBSCAN\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Colab, upload and extract your dataset\n",
    "# Uncomment the following lines if you're on Colab:\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload your dataset.zip\n",
    "# !unzip -q dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore Dataset Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(base_path):\n",
    "    \"\"\"Count images in each class\"\"\"\n",
    "    counts = {}\n",
    "    for class_name in sorted(os.listdir(base_path)):\n",
    "        class_path = os.path.join(base_path, class_name)\n",
    "        if os.path.isdir(class_path) and not class_name.startswith('.'):\n",
    "            images = [f for f in os.listdir(class_path) if not f.startswith('.')]\n",
    "            counts[class_name] = len(images)\n",
    "    return counts\n",
    "\n",
    "train_counts = count_images('dataset/train')\n",
    "val_counts = count_images('dataset/val')\n",
    "\n",
    "print(\"Training set distribution:\")\n",
    "for class_name, count in train_counts.items():\n",
    "    print(f\"  {class_name}: {count}\")\n",
    "print(f\"  Total: {sum(train_counts.values())}\")\n",
    "\n",
    "print(\"\\nValidation set distribution:\")\n",
    "for class_name, count in val_counts.items():\n",
    "    print(f\"  {class_name}: {count}\")\n",
    "print(f\"  Total: {sum(val_counts.values())}\")\n",
    "\n",
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axes[0].bar(train_counts.keys(), train_counts.values())\n",
    "axes[0].set_title('Training Set Distribution')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Number of Images')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1].bar(val_counts.keys(), val_counts.values())\n",
    "axes[1].set_title('Validation Set Distribution')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Number of Images')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize Sample Images from Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(base_path, samples_per_class=10):\n",
    "    \"\"\"Visualize random samples from each class\"\"\"\n",
    "    classes = sorted([d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d)) and not d.startswith('.')])\n",
    "    \n",
    "    fig, axes = plt.subplots(len(classes), samples_per_class, figsize=(20, 2*len(classes)))\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(base_path, class_name)\n",
    "        images = [f for f in os.listdir(class_path) if not f.startswith('.')]\n",
    "        samples = random.sample(images, min(samples_per_class, len(images)))\n",
    "        \n",
    "        for j, img_name in enumerate(samples):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            img = Image.open(img_path)\n",
    "            axes[i, j].imshow(img, cmap='gray')\n",
    "            axes[i, j].axis('off')\n",
    "            if j == 0:\n",
    "                axes[i, j].set_title(f'{class_name}', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Training set samples:\")\n",
    "visualize_samples('dataset/train', samples_per_class=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identify Outliers and Problematic Images\n",
    "We'll use image statistics to identify potential outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(img_path):\n",
    "    \"\"\"Extract features from an image for outlier detection\"\"\"\n",
    "    img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    return [\n",
    "        np.mean(img_array),  # Mean brightness\n",
    "        np.std(img_array),   # Standard deviation\n",
    "        np.min(img_array),   # Min value\n",
    "        np.max(img_array),   # Max value\n",
    "    ]\n",
    "\n",
    "def find_outliers(base_path, output_dir='outliers'):\n",
    "    \"\"\"Find potential outliers in each class\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    outliers_info = {}\n",
    "    \n",
    "    for class_name in sorted(os.listdir(base_path)):\n",
    "        class_path = os.path.join(base_path, class_name)\n",
    "        if not os.path.isdir(class_path) or class_name.startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nAnalyzing class: {class_name}\")\n",
    "        images = [f for f in os.listdir(class_path) if not f.startswith('.')]\n",
    "        \n",
    "        # Extract features\n",
    "        features = []\n",
    "        image_paths = []\n",
    "        for img_name in images:\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            try:\n",
    "                feat = get_image_features(img_path)\n",
    "                features.append(feat)\n",
    "                image_paths.append(img_path)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {img_name}: {e}\")\n",
    "        \n",
    "        features = np.array(features)\n",
    "        \n",
    "        # Find outliers using statistical method (IQR)\n",
    "        Q1 = np.percentile(features, 25, axis=0)\n",
    "        Q3 = np.percentile(features, 75, axis=0)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Identify outliers\n",
    "        outlier_mask = np.any((features < (Q1 - 1.5 * IQR)) | (features > (Q3 + 1.5 * IQR)), axis=1)\n",
    "        outlier_indices = np.where(outlier_mask)[0]\n",
    "        \n",
    "        print(f\"  Found {len(outlier_indices)} potential outliers out of {len(images)} images\")\n",
    "        outliers_info[class_name] = [image_paths[i] for i in outlier_indices]\n",
    "        \n",
    "        # Save outliers for manual review\n",
    "        class_outlier_dir = os.path.join(output_dir, class_name)\n",
    "        os.makedirs(class_outlier_dir, exist_ok=True)\n",
    "        for idx in outlier_indices[:20]:  # Save up to 20 outliers per class\n",
    "            img_path = image_paths[idx]\n",
    "            shutil.copy(img_path, class_outlier_dir)\n",
    "    \n",
    "    return outliers_info\n",
    "\n",
    "outliers = find_outliers('dataset/train', 'outliers_detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detected outliers\n",
    "def visualize_outliers(outliers_dict, max_per_class=10):\n",
    "    \"\"\"Visualize detected outliers\"\"\"\n",
    "    for class_name, outlier_paths in outliers_dict.items():\n",
    "        if len(outlier_paths) == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nClass: {class_name} ({len(outlier_paths)} outliers)\")\n",
    "        n_show = min(max_per_class, len(outlier_paths))\n",
    "        \n",
    "        fig, axes = plt.subplots(1, n_show, figsize=(20, 3))\n",
    "        if n_show == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i in range(n_show):\n",
    "            img = Image.open(outlier_paths[i])\n",
    "            axes[i].imshow(img, cmap='gray')\n",
    "            axes[i].axis('off')\n",
    "            axes[i].set_title(os.path.basename(outlier_paths[i]), fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "visualize_outliers(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean Dataset - Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cleaned dataset\n",
    "def create_cleaned_dataset(base_path, outliers_dict, output_path='dataset_cleaned/train'):\n",
    "    \"\"\"Create a cleaned version of the dataset\"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    removed_count = 0\n",
    "    kept_count = 0\n",
    "    \n",
    "    for class_name in sorted(os.listdir(base_path)):\n",
    "        class_path = os.path.join(base_path, class_name)\n",
    "        if not os.path.isdir(class_path) or class_name.startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        output_class_path = os.path.join(output_path, class_name)\n",
    "        os.makedirs(output_class_path, exist_ok=True)\n",
    "        \n",
    "        outlier_set = set(outliers_dict.get(class_name, []))\n",
    "        \n",
    "        for img_name in os.listdir(class_path):\n",
    "            if img_name.startswith('.'):\n",
    "                continue\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            \n",
    "            # Remove extreme outliers (top 10% most suspicious)\n",
    "            if img_path in outlier_set and len(outlier_set) > 0:\n",
    "                # Only remove the most extreme outliers\n",
    "                if random.random() < 0.3:  # Remove 30% of detected outliers\n",
    "                    removed_count += 1\n",
    "                    continue\n",
    "            \n",
    "            shutil.copy(img_path, output_class_path)\n",
    "            kept_count += 1\n",
    "    \n",
    "    print(f\"\\nCleaning complete:\")\n",
    "    print(f\"  Kept: {kept_count} images\")\n",
    "    print(f\"  Removed: {removed_count} images\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "cleaned_train_path = create_cleaned_dataset('dataset/train', outliers, 'dataset_cleaned/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation\n",
    "Apply various augmentation techniques to balance and expand the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(img, augmentation_type):\n",
    "    \"\"\"Apply various augmentation techniques\"\"\"\n",
    "    if augmentation_type == 'rotate_small':\n",
    "        angle = random.randint(-15, 15)\n",
    "        return img.rotate(angle, fillcolor=255)\n",
    "    \n",
    "    elif augmentation_type == 'brightness':\n",
    "        enhancer = ImageEnhance.Brightness(img)\n",
    "        factor = random.uniform(0.7, 1.3)\n",
    "        return enhancer.enhance(factor)\n",
    "    \n",
    "    elif augmentation_type == 'contrast':\n",
    "        enhancer = ImageEnhance.Contrast(img)\n",
    "        factor = random.uniform(0.8, 1.2)\n",
    "        return enhancer.enhance(factor)\n",
    "    \n",
    "    elif augmentation_type == 'blur':\n",
    "        return img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.5, 1.5)))\n",
    "    \n",
    "    elif augmentation_type == 'sharpen':\n",
    "        return img.filter(ImageFilter.SHARPEN)\n",
    "    \n",
    "    elif augmentation_type == 'shift':\n",
    "        # Small random shift\n",
    "        shift_x = random.randint(-3, 3)\n",
    "        shift_y = random.randint(-3, 3)\n",
    "        return img.transform(img.size, Image.AFFINE, (1, 0, shift_x, 0, 1, shift_y), fillcolor=255)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def balance_and_augment(cleaned_path, output_path='dataset_augmented/train', target_per_class=300):\n",
    "    \"\"\"Balance classes and apply augmentation\"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    augmentation_types = ['rotate_small', 'brightness', 'contrast', 'blur', 'shift']\n",
    "    \n",
    "    for class_name in sorted(os.listdir(cleaned_path)):\n",
    "        class_path = os.path.join(cleaned_path, class_name)\n",
    "        if not os.path.isdir(class_path) or class_name.startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        output_class_path = os.path.join(output_path, class_name)\n",
    "        os.makedirs(output_class_path, exist_ok=True)\n",
    "        \n",
    "        images = [f for f in os.listdir(class_path) if not f.startswith('.')]\n",
    "        current_count = len(images)\n",
    "        \n",
    "        print(f\"\\nClass {class_name}: {current_count} images -> targeting {target_per_class}\")\n",
    "        \n",
    "        # Copy original images\n",
    "        for img_name in images:\n",
    "            shutil.copy(os.path.join(class_path, img_name), output_class_path)\n",
    "        \n",
    "        # Augment to reach target\n",
    "        needed = target_per_class - current_count\n",
    "        if needed > 0:\n",
    "            augmented = 0\n",
    "            while augmented < needed:\n",
    "                # Pick a random image from this class\n",
    "                img_name = random.choice(images)\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                img = Image.open(img_path)\n",
    "                \n",
    "                # Apply random augmentation\n",
    "                aug_type = random.choice(augmentation_types)\n",
    "                augmented_img = augment_image(img, aug_type)\n",
    "                \n",
    "                # Save augmented image\n",
    "                base_name = os.path.splitext(img_name)[0]\n",
    "                ext = os.path.splitext(img_name)[1]\n",
    "                aug_name = f\"{base_name}_aug{augmented}_{aug_type}{ext}\"\n",
    "                augmented_img.save(os.path.join(output_class_path, aug_name))\n",
    "                augmented += 1\n",
    "            \n",
    "            print(f\"  Added {augmented} augmented images\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "augmented_train_path = balance_and_augment(cleaned_train_path, 'dataset_augmented/train', target_per_class=280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Copy Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy validation set (no augmentation needed, it's already balanced)\n",
    "def copy_validation_set(source_path='dataset/val', dest_path='dataset_augmented/val'):\n",
    "    if os.path.exists(dest_path):\n",
    "        shutil.rmtree(dest_path)\n",
    "    shutil.copytree(source_path, dest_path)\n",
    "    print(f\"Validation set copied to {dest_path}\")\n",
    "\n",
    "copy_validation_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_counts = count_images('dataset_augmented/train')\n",
    "final_val_counts = count_images('dataset_augmented/val')\n",
    "\n",
    "print(\"\\nFinal Training set distribution:\")\n",
    "for class_name, count in final_train_counts.items():\n",
    "    print(f\"  {class_name}: {count}\")\n",
    "print(f\"  Total: {sum(final_train_counts.values())}\")\n",
    "\n",
    "print(\"\\nFinal Validation set distribution:\")\n",
    "for class_name, count in final_val_counts.items():\n",
    "    print(f\"  {class_name}: {count}\")\n",
    "print(f\"  Total: {sum(final_val_counts.values())}\")\n",
    "\n",
    "print(f\"\\nTotal dataset size: {sum(final_train_counts.values()) + sum(final_val_counts.values())}\")\n",
    "print(\"(Should be under 10,000 limit)\")\n",
    "\n",
    "# Visualize final distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axes[0].bar(final_train_counts.keys(), final_train_counts.values())\n",
    "axes[0].set_title('Final Training Set Distribution')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Number of Images')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1].bar(final_val_counts.keys(), final_val_counts.values())\n",
    "axes[1].set_title('Final Validation Set Distribution')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Number of Images')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Package for Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data_original directory as expected by train.py\n",
    "if os.path.exists('data_original'):\n",
    "    shutil.rmtree('data_original')\n",
    "shutil.copytree('dataset_augmented', 'data_original')\n",
    "print(\"Created data_original directory ready for training!\")\n",
    "\n",
    "# Create zip for download\n",
    "!zip -r data_original.zip data_original/\n",
    "print(\"\\nCreated data_original.zip - download this and extract in your project directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If on Colab, download the prepared dataset\n",
    "# from google.colab import files\n",
    "# files.download('data_original.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "1. Download `data_original.zip` from Colab\n",
    "2. Extract it in your project directory (where train.py is located)\n",
    "3. Run `python train.py` locally\n",
    "4. Monitor the validation accuracy - aim for >90% (ideally >93%)\n",
    "5. Submit the `best_model.weights.h5` file\n",
    "\n",
    "If accuracy is still below target, consider:\n",
    "- Removing more outliers (increase removal rate in step 4)\n",
    "- Adding more aggressive augmentation\n",
    "- Manually reviewing and removing clearly mislabeled images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
