{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMERGENCY FIX - Overfitting Issue\n",
    "**New Strategy: Less aggressive cleaning + Strong regularization + Model improvements**\n",
    "\n",
    "## Problem:\n",
    "- Training: 98%+\n",
    "- Validation: 53-68%\n",
    "- Gap: 30-45%\n",
    "\n",
    "## Root Cause:\n",
    "Too aggressive cleaning removed good data, leaving noisy samples that the model memorizes.\n",
    "\n",
    "## New Approach:\n",
    "1. **Minimal cleaning** (remove only extreme outliers)\n",
    "2. **Moderate augmentation** (avoid unrealistic samples)\n",
    "3. **Strong regularization** (dropout, L2 weight decay)\n",
    "4. **Longer training** with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"Upload dataset.zip:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "\n",
    "print(\"âœ“ Extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "print(\"TF:\", tf.__version__)\n",
    "print(\"GPU:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MINIMAL CLEANING - Remove only EXTREME outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(base_path):\n",
    "    counts = {}\n",
    "    for class_name in sorted(os.listdir(base_path)):\n",
    "        class_path = os.path.join(base_path, class_name)\n",
    "        if os.path.isdir(class_path) and not class_name.startswith('.'):\n",
    "            counts[class_name] = len([f for f in os.listdir(class_path) if not f.startswith('.')])\n",
    "    return counts\n",
    "\n",
    "def get_image_stats(img_path):\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        arr = np.array(img)\n",
    "        return [np.mean(arr), np.std(arr)]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def find_extreme_outliers_only(base_path):\n",
    "    \"\"\"Only remove the MOST extreme outliers (top 5% worst)\"\"\"\n",
    "    outliers_info = {}\n",
    "    \n",
    "    for class_name in sorted(os.listdir(base_path)):\n",
    "        class_path = os.path.join(base_path, class_name)\n",
    "        if not os.path.isdir(class_path) or class_name.startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        images = [f for f in os.listdir(class_path) if not f.startswith('.')]\n",
    "        features = []\n",
    "        paths = []\n",
    "        \n",
    "        for img_name in images:\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            feat = get_image_stats(img_path)\n",
    "            if feat:\n",
    "                features.append(feat)\n",
    "                paths.append(img_path)\n",
    "        \n",
    "        features = np.array(features)\n",
    "        median = np.median(features, axis=0)\n",
    "        \n",
    "        # Calculate distance from median\n",
    "        distances = np.sqrt(np.sum((features - median)**2, axis=1))\n",
    "        \n",
    "        # Only flag top 5% most extreme\n",
    "        threshold = np.percentile(distances, 95)\n",
    "        extreme_indices = np.where(distances > threshold)[0]\n",
    "        \n",
    "        outliers_info[class_name] = [paths[i] for i in extreme_indices]\n",
    "        print(f\"Class {class_name}: {len(extreme_indices)} extreme outliers (top 5%)\")\n",
    "    \n",
    "    return outliers_info\n",
    "\n",
    "print(\"Finding EXTREME outliers only...\")\n",
    "outliers = find_extreme_outliers_only('dataset/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_clean(base_path, outliers_dict, output_path):\n",
    "    \"\"\"Remove only extreme outliers (not all detected ones)\"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    removed = 0\n",
    "    kept = 0\n",
    "    \n",
    "    for class_name in sorted(os.listdir(base_path)):\n",
    "        class_path = os.path.join(base_path, class_name)\n",
    "        if not os.path.isdir(class_path) or class_name.startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        output_class_path = os.path.join(output_path, class_name)\n",
    "        os.makedirs(output_class_path, exist_ok=True)\n",
    "        outlier_set = set(outliers_dict.get(class_name, []))\n",
    "        \n",
    "        for img_name in os.listdir(class_path):\n",
    "            if img_name.startswith('.'):\n",
    "                continue\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            \n",
    "            # Remove ALL extreme outliers (they're already the worst 5%)\n",
    "            if img_path in outlier_set:\n",
    "                removed += 1\n",
    "                continue\n",
    "            \n",
    "            shutil.copy(img_path, output_class_path)\n",
    "            kept += 1\n",
    "    \n",
    "    print(f\"\\nMinimal cleaning: Kept {kept}, Removed {removed}\")\n",
    "    return output_path\n",
    "\n",
    "cleaned = minimal_clean('dataset/train', outliers, 'cleaned/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODERATE AUGMENTATION - Realistic transforms only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moderate_augment(img, aug_type):\n",
    "    \"\"\"Conservative augmentation - keep samples realistic\"\"\"\n",
    "    if aug_type == 'rotate':\n",
    "        return img.rotate(random.randint(-12, 12), fillcolor=255)\n",
    "    elif aug_type == 'brightness':\n",
    "        return ImageEnhance.Brightness(img).enhance(random.uniform(0.8, 1.2))\n",
    "    elif aug_type == 'contrast':\n",
    "        return ImageEnhance.Contrast(img).enhance(random.uniform(0.85, 1.15))\n",
    "    elif aug_type == 'shift':\n",
    "        shift_x, shift_y = random.randint(-2, 2), random.randint(-2, 2)\n",
    "        return img.transform(img.size, Image.AFFINE, (1, 0, shift_x, 0, 1, shift_y), fillcolor=255)\n",
    "    return img\n",
    "\n",
    "def balance_moderate(cleaned_path, output_path, target=260):\n",
    "    \"\"\"Balance with moderate, realistic augmentation\"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    aug_types = ['rotate', 'brightness', 'contrast', 'shift']\n",
    "    \n",
    "    for class_name in sorted(os.listdir(cleaned_path)):\n",
    "        class_path = os.path.join(cleaned_path, class_name)\n",
    "        if not os.path.isdir(class_path) or class_name.startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        output_class_path = os.path.join(output_path, class_name)\n",
    "        os.makedirs(output_class_path, exist_ok=True)\n",
    "        images = [f for f in os.listdir(class_path) if not f.startswith('.')]\n",
    "        \n",
    "        # Copy originals\n",
    "        for img_name in images:\n",
    "            shutil.copy(os.path.join(class_path, img_name), output_class_path)\n",
    "        \n",
    "        # Moderate augmentation\n",
    "        needed = target - len(images)\n",
    "        if needed > 0:\n",
    "            for i in range(needed):\n",
    "                img_name = random.choice(images)\n",
    "                img = Image.open(os.path.join(class_path, img_name))\n",
    "                aug_img = moderate_augment(img, random.choice(aug_types))\n",
    "                aug_img.save(os.path.join(output_class_path, f\"aug_{i}_{img_name}\"))\n",
    "        \n",
    "        print(f\"Class {class_name}: {len(images)} â†’ {target} (+{max(0, needed)})\")\n",
    "\n",
    "print(\"\\nBalancing with MODERATE augmentation...\")\n",
    "balance_moderate(cleaned, 'augmented/train', target=260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy val and create data_original\n",
    "shutil.copytree('dataset/val', 'augmented/val', dirs_exist_ok=True)\n",
    "if os.path.exists('data_original'):\n",
    "    shutil.rmtree('data_original')\n",
    "shutil.copytree('augmented', 'data_original')\n",
    "\n",
    "print(\"\\nâœ“ Data ready\")\n",
    "print(f\"Train: {sum(count_images('data_original/train').values())}\")\n",
    "print(f\"Val: {sum(count_images('data_original/val').values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPROVED MODEL with Strong Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "batch_size = 8\n",
    "\n",
    "train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"data_original/train\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(32, 32),\n",
    ")\n",
    "\n",
    "valid = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"data_original/val\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(32, 32),\n",
    ")\n",
    "\n",
    "print(f\"Batches - Train: {train.cardinality().numpy()}, Val: {valid.cardinality().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with STRONG REGULARIZATION\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    input_shape=(32, 32, 3),\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    ")\n",
    "base_model = tf.keras.Model(\n",
    "    base_model.inputs, outputs=[base_model.get_layer(\"conv2_block3_out\").output]\n",
    ")\n",
    "\n",
    "inputs = tf.keras.Input(shape=(32, 32, 3))\n",
    "x = tf.keras.applications.resnet.preprocess_input(inputs)\n",
    "x = base_model(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x[0])\n",
    "\n",
    "# ADD STRONG REGULARIZATION\n",
    "x = tf.keras.layers.Dropout(0.5)(x)  # Drop 50% of neurons\n",
    "x = tf.keras.layers.Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # L2 regularization\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)  # Another dropout\n",
    "x = tf.keras.layers.Dense(10)(x)  # Output\n",
    "\n",
    "model = tf.keras.Model(inputs, x)\n",
    "\n",
    "# Use learning rate schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "print(\"\\nModel with strong regularization:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with early stopping\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"best_model.weights.h5\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING with Regularization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=valid,\n",
    "    epochs=60,\n",
    "    callbacks=[checkpoint, early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.load_weights(\"best_model.weights.h5\")\n",
    "loss, acc = model.evaluate(valid)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"FINAL VALIDATION ACCURACY: {acc*100:.2f}%\")\n",
    "print(f\"FINAL TRAINING ACCURACY: {history.history['accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"OVERFITTING GAP: {(history.history['accuracy'][-1] - acc)*100:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if acc >= 0.93:\n",
    "    print(\"\\nðŸŽ‰ BONUS! â‰¥93%\")\n",
    "elif acc >= 0.90:\n",
    "    print(f\"\\nâœ“ SUCCESS! â‰¥90%\")\n",
    "else:\n",
    "    print(f\"\\nâš  Need {(0.90-acc)*100:.2f}% more\")\n",
    "\n",
    "model.save_weights(\"submission.weights.h5\")\n",
    "print(\"\\nSaved: submission.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Val', linewidth=2)\n",
    "plt.axhline(y=0.90, color='r', linestyle='--', alpha=0.7)\n",
    "plt.title('Accuracy - Overfitting Check', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Val', linewidth=2)\n",
    "plt.title('Loss', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show gap over time\n",
    "gap = [train_acc - val_acc for train_acc, val_acc in zip(history.history['accuracy'], history.history['val_accuracy'])]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(gap, linewidth=2, color='red')\n",
    "plt.title('Overfitting Gap (Train - Val)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy Gap')\n",
    "plt.xlabel('Epoch')\n",
    "plt.axhline(y=0.1, color='orange', linestyle='--', label='10% gap', alpha=0.7)\n",
    "plt.axhline(y=0.2, color='red', linestyle='--', label='20% gap', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal gap: {gap[-1]*100:.2f}%\")\n",
    "if gap[-1] < 0.15:\n",
    "    print(\"âœ“ Good generalization!\")\n",
    "else:\n",
    "    print(\"âš  Still overfitting - consider more regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download\n",
    "from google.colab import files\n",
    "files.download('best_model.weights.h5')\n",
    "files.download('submission.weights.h5')\n",
    "print(\"âœ“ Downloaded\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
