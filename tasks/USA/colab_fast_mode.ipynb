{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Audio Transcription - FAST MODE ‚ö°\n",
    "### Optimized for speed with tiny model and smart keyword search\n",
    "\n",
    "**Instructions:**\n",
    "1. Upload audio_task_43.mp3 to your Google Drive\n",
    "2. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
    "3. Run all cells\n",
    "\n",
    "**Speed optimizations:**\n",
    "- Uses 'tiny' Whisper model (5-10x faster, slightly less accurate)\n",
    "- 10-minute chunks (fewer chunks = less overhead)\n",
    "- Skips already-transcribed chunks (safe to restart)\n",
    "- Smart keyword search (looks for 'keyword' word first)\n",
    "- Forces English transcription for multilingual audio\n",
    "\n",
    "**Estimated time:** ~15-25 minutes (vs 60-90 with base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Whisper\n",
    "!pip install -q openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö† WARNING: Enable GPU in Runtime ‚Üí Change runtime type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Update this path if needed\n",
    "audio_file = \"/content/drive/MyDrive/audio_task_43.mp3\"\n",
    "\n",
    "if os.path.exists(audio_file):\n",
    "    print(f\"‚úì Found: {audio_file}\")\n",
    "    print(f\"  Size: {os.path.getsize(audio_file) / (1024**2):.1f} MB\")\n",
    "else:\n",
    "    print(f\"‚ö† Searching for audio_task_43.mp3...\")\n",
    "    import subprocess\n",
    "    result = subprocess.run(['find', '/content/drive/MyDrive', '-name', 'audio_task_43.mp3'],\n",
    "                          capture_output=True, text=True, timeout=60)\n",
    "    files = [f for f in result.stdout.strip().split('\\n') if f]\n",
    "    if files:\n",
    "        audio_file = files[0]\n",
    "        print(f\"‚úì Found at: {audio_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - OPTIMIZED FOR SPEED\n",
    "CHUNK_DURATION_MIN = 10  # 10-minute chunks (larger = faster)\n",
    "TOTAL_DURATION_MIN = 470  # 7h50min\n",
    "\n",
    "import whisper\n",
    "import subprocess\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('audio_chunks', exist_ok=True)\n",
    "os.makedirs('transcriptions', exist_ok=True)\n",
    "print(\"‚úì Created output directories: audio_chunks/ and transcriptions/\")\n",
    "\n",
    "# Load TINY model for speed\n",
    "print(\"Loading Whisper 'tiny' model (FAST MODE)...\")\n",
    "model = whisper.load_model(\"tiny\")\n",
    "print(f\"‚úì Model on {'GPU' if next(model.parameters()).is_cuda else 'CPU'}\")\n",
    "\n",
    "num_chunks = (TOTAL_DURATION_MIN + CHUNK_DURATION_MIN - 1) // CHUNK_DURATION_MIN\n",
    "print(f\"\\nWill process {num_chunks} chunks of {CHUNK_DURATION_MIN} minutes each\")\n",
    "print(f\"‚ö° FAST MODE: Using tiny model + large chunks for 5-10x speedup\")\n",
    "print(f\"üîë Forcing English transcription for multilingual audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMART keyword search - looks for \"keyword\" word first, then validates pattern\n",
    "ordinal_map = {\n",
    "    'first': 1, '1st': 1, 'second': 2, '2nd': 2, 'third': 3, '3rd': 3,\n",
    "    'fourth': 4, '4th': 4, 'fifth': 5, '5th': 5, 'sixth': 6, '6th': 6,\n",
    "    'seventh': 7, '7th': 7, 'eighth': 8, '8th': 8, 'ninth': 9, '9th': 9,\n",
    "    'tenth': 10, '10th': 10, 'eleventh': 11, '11th': 11, 'twelfth': 12, '12th': 12,\n",
    "    'thirteenth': 13, '13th': 13, 'fourteenth': 14, '14th': 14,\n",
    "    'fifteenth': 15, '15th': 15, 'sixteenth': 16, '16th': 16,\n",
    "    'seventeenth': 17, '17th': 17, 'eighteenth': 18, '18th': 18,\n",
    "    'nineteenth': 19, '19th': 19, 'twentieth': 20, '20th': 20\n",
    "}\n",
    "\n",
    "def parse_ordinal(ordinal_str):\n",
    "    \"\"\"Parse ordinal to number\"\"\"\n",
    "    ordinal_lower = ordinal_str.lower()\n",
    "    if ordinal_lower in ordinal_map:\n",
    "        return ordinal_map[ordinal_lower]\n",
    "    \n",
    "    # Try to extract pure number\n",
    "    num_match = re.match(r'(\\d+)', ordinal_str)\n",
    "    if num_match:\n",
    "        return int(num_match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "def find_keywords(text):\n",
    "    \"\"\"\n",
    "    SMART SEARCH: Find 'keyword' word first, then validate pattern around it.\n",
    "    More reliable than searching for full pattern.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Find all occurrences of \"keyword\" (allowing minor misspellings)\n",
    "    keyword_pattern = r'\\bkey\\s*wo?r?d\\b'\n",
    "    \n",
    "    for match in re.finditer(keyword_pattern, text, re.IGNORECASE):\n",
    "        # Get context around \"keyword\" (¬±150 chars)\n",
    "        start = max(0, match.start() - 150)\n",
    "        end = min(len(text), match.end() + 150)\n",
    "        context = text[start:end]\n",
    "        \n",
    "        # Look for pattern: [ordinal] letter [in/of] keyword is [letter], [phonetic]\n",
    "        pattern = r'(\\w+)\\s+letter\\s+(?:in|of)\\s+(?:the\\s+)?key\\s*wo?r?d\\s+is\\s+([A-Z])[,.\\s]+([A-Za-z]+)'\n",
    "        \n",
    "        matches = re.findall(pattern, context, re.IGNORECASE)\n",
    "        \n",
    "        for ordinal, letter, phonetic in matches:\n",
    "            pos_num = parse_ordinal(ordinal)\n",
    "            if pos_num:\n",
    "                results.append((pos_num, letter.upper(), phonetic))\n",
    "        \n",
    "        # If no match with pattern, show context for manual inspection\n",
    "        if not matches:\n",
    "            print(f\"    ‚ö† Found 'keyword' but pattern unclear:\")\n",
    "            print(f\"      ...{context[max(0, match.start()-start-50):match.end()-start+50]}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úì Smart keyword search ready (looks for 'keyword' word first)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all chunks - FAST MODE with skip logic\n",
    "all_keywords = {}\n",
    "keyword_locations = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Processing {num_chunks} chunks - FAST MODE ‚ö°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "start_time = time.time()\n",
    "transcribed_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start_min = i * CHUNK_DURATION_MIN\n",
    "    start_sec = start_min * 60\n",
    "    duration_sec = CHUNK_DURATION_MIN * 60\n",
    "    \n",
    "    chunk_file = f\"audio_chunks/chunk_{i:03d}.mp3\"\n",
    "    transcript_file = f\"transcriptions/chunk_{i:03d}.txt\"\n",
    "    \n",
    "    print(f\"\\n[{i+1}/{num_chunks}] Minutes {start_min}-{start_min+CHUNK_DURATION_MIN}\")\n",
    "    \n",
    "    # Extract chunk (skip if already exists)\n",
    "    if not os.path.exists(chunk_file):\n",
    "        cmd = ['ffmpeg', '-y', '-v', 'quiet', '-ss', str(start_sec),\n",
    "               '-i', audio_file, '-t', str(duration_sec),\n",
    "               '-acodec', 'libmp3lame', chunk_file]\n",
    "        subprocess.run(cmd, check=True)\n",
    "    \n",
    "    # OPTIMIZATION: Skip if already transcribed\n",
    "    if os.path.exists(transcript_file):\n",
    "        print(f\"  ‚è≠ Skipping transcription (already exists)\")\n",
    "        with open(transcript_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            # Extract just the transcript (skip header)\n",
    "            parts = content.split('='*70)\n",
    "            transcript = parts[-1].strip() if len(parts) > 1 else content\n",
    "        skipped_count += 1\n",
    "    else:\n",
    "        # Transcribe with FORCED ENGLISH and TINY model (FAST!)\n",
    "        result = model.transcribe(chunk_file, language='en', verbose=False, fp16=torch.cuda.is_available())\n",
    "        transcript = result[\"text\"]\n",
    "        \n",
    "        # Save transcription\n",
    "        with open(transcript_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Chunk {i} - Minutes {start_min}-{start_min+CHUNK_DURATION_MIN}\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            f.write(transcript)\n",
    "        \n",
    "        transcribed_count += 1\n",
    "    \n",
    "    # Search for keywords using SMART search\n",
    "    matches = find_keywords(transcript)\n",
    "    \n",
    "    if matches:\n",
    "        print(f\"  ‚úì‚úì‚úì FOUND {len(matches)} KEYWORD(S)! ‚úì‚úì‚úì\")\n",
    "        for pos_num, letter, phonetic in matches:\n",
    "            all_keywords[pos_num] = letter\n",
    "            keyword_locations[pos_num] = {\n",
    "                'letter': letter,\n",
    "                'phonetic': phonetic,\n",
    "                'chunk': i,\n",
    "                'time_min': start_min\n",
    "            }\n",
    "            print(f\"      Position {pos_num}: {letter} ({phonetic})\")\n",
    "    else:\n",
    "        snippet = transcript[:80].replace('\\n', ' ')\n",
    "        print(f\"  - No keywords (sample: {snippet}...)\")\n",
    "    \n",
    "    # Progress every 5 chunks\n",
    "    if (i + 1) % 5 == 0:\n",
    "        elapsed = (time.time() - start_time) / 60\n",
    "        avg_time = elapsed / (i + 1) if (i + 1) > 0 else 0\n",
    "        remaining = avg_time * (num_chunks - i - 1)\n",
    "        print(f\"\\n  ‚è± Progress: {i+1}/{num_chunks} | {elapsed:.1f}m elapsed | ~{remaining:.1f}m remaining\")\n",
    "        print(f\"  üîë Keywords found: {len(all_keywords)} | Transcribed: {transcribed_count} | Skipped: {skipped_count}\")\n",
    "\n",
    "total_time = (time.time() - start_time) / 60\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úì Completed in {total_time:.1f} minutes\")\n",
    "print(f\"‚úì Transcribed {transcribed_count} chunks, skipped {skipped_count}\")\n",
    "print(f\"‚úì Audio chunks saved to: audio_chunks/\")\n",
    "print(f\"‚úì Transcriptions saved to: transcriptions/\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if all_keywords:\n",
    "    print(f\"\\n‚úì Found {len(all_keywords)} keyword letters:\\n\")\n",
    "    \n",
    "    for pos in sorted(all_keywords.keys()):\n",
    "        loc = keyword_locations[pos]\n",
    "        print(f\"  Position {pos}: {loc['letter']} ({loc['phonetic']}) - at {loc['time_min']} min\")\n",
    "    \n",
    "    # Build keyword\n",
    "    max_pos = max(all_keywords.keys())\n",
    "    keyword = \"\"\n",
    "    for i in range(1, max_pos + 1):\n",
    "        keyword += all_keywords.get(i, \"_\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üö© KEYWORD: {keyword}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Check completeness\n",
    "    missing = [i for i in range(1, max_pos + 1) if i not in all_keywords]\n",
    "    if missing:\n",
    "        print(f\"\\n‚ö† Missing positions: {missing}\")\n",
    "        print(\"\\nTo manually search transcriptions for missing letters:\")\n",
    "        print(\"  !grep -i 'keyword' transcriptions/*.txt\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì‚úì‚úì COMPLETE KEYWORD! ‚úì‚úì‚úì\")\n",
    "    \n",
    "    # Save solution\n",
    "    with open('SOLUTION.txt', 'w') as f:\n",
    "        f.write(f\"KEYWORD: {keyword}\\n\\n\")\n",
    "        f.write(\"Letters found:\\n\")\n",
    "        for pos in sorted(all_keywords.keys()):\n",
    "            loc = keyword_locations[pos]\n",
    "            f.write(f\"  Position {pos}: {loc['letter']} ({loc['phonetic']}) at {loc['time_min']} min\\n\")\n",
    "        if missing:\n",
    "            f.write(f\"\\nMissing positions: {missing}\\n\")\n",
    "    \n",
    "    print(\"\\n‚úì Solution saved to SOLUTION.txt\")\n",
    "    \n",
    "    # Download result\n",
    "    from google.colab import files\n",
    "    files.download('SOLUTION.txt')\n",
    "    print(\"\\n‚úì Downloaded SOLUTION.txt to your computer!\")\n",
    "else:\n",
    "    print(\"\\n‚ö† No keywords found. Manually search transcriptions:\")\n",
    "    print(\"  !grep -i 'keyword' transcriptions/*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Search (if needed)\n",
    "\n",
    "If some keywords are missing, search manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual search for \"keyword\" in all transcriptions\n",
    "!grep -i 'keyword' transcriptions/*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all transcriptions as zip\n",
    "!zip -r transcriptions.zip transcriptions/\n",
    "from google.colab import files\n",
    "files.download('transcriptions.zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
